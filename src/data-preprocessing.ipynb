{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos para poder ser empleados por la _RNA_\n",
    "\n",
    "Los datos empleados en el proyecto se encuentran en el directorio _\"/data/\"_, de forma que se dividen en puros (raw) y clasificados (classified):\n",
    "\n",
    "```\n",
    ".\n",
    "|-data\n",
    "   |- raw_data -> ficheros de datos sin procesar (procesado preliminar).\n",
    "   |     |- sensores -> ficheros correspondientes a los datos de los sensores.\n",
    "   |     |- pluviometro -> ficheros correspondientes a los datos del pluviómetro.\n",
    "   |- classified_data -> ficheros de datos procesados.\n",
    "```\n",
    "\n",
    "Importamos las librerías para el procesamiento de los datos.\n",
    "\n",
    "Para la ETL (extracción, transformación y carga) emplearemos Pandas.\n",
    "\n",
    "Por otro lado, en el caso de los datos del pluviómetro para comprobar los valores anómalos emplearemos la librería nativa json utilizando datos de la estación meteorológica de la AEMET situada en Aranda de Duero, ya que es la que más próxima se encuentra al viñedo (situado en el municipio de Anguix, Burgos).\n",
    "El contenido de los directorios _\"/data/raw_data/sensores\"_ y _\"/data/raw_data/pluviometro\"_  será listado mediante el módulo nativo de Python _\"os\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las constantes para establecer la dirección en la que se encuentran los ficheros de datos y el directorio en el que se deberá almacenar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de directorios\n",
    "from env_paths import *\n",
    "from global_variables import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección los valores atípicos\n",
    "La _\"detección de valores atípicos\"_ permite detectar los valores anómalos (observaciones numéricamente diferentes al resto) en los datos debidos a diversos factores como errores humanos, mecánicos extremos de lecturas genuinas o por reemplazos de valores perdidos (_missing data_).\n",
    "\n",
    "En este caso se empleará el análisis de _outliers_ por medio de la detección de valores atípicos mediante una ventana móvil establecida agrupando las muestras correspondientes al mismo día, de forma que por cada grupo se calcula su media, desviación típica y mediana.\n",
    "Se establece un umbral (_threshold_) de X veces la desviación estándar, siendo aquellas muestras cuya diferencia con la media del grupo que se encuentren por encima de este umbral modificadas estableciendo la mediana de la variable correspondiente en el análisis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(data, t_times):\n",
    "    # Ventana de datos agrupados por dia\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "\n",
    "    for _, group in rolling_win:\n",
    "        # Para cada columna realizamos el analisis por deteccion de valores atipicos\n",
    "        for col in data.columns:\n",
    "            if(col != 'ts'):\n",
    "                # Calculo de la media y la desviacion tipica del grupo\n",
    "                group_mean = group[col].mean()\n",
    "                group_std = group[col].std()\n",
    "                group_median = group[col].median()\n",
    "\n",
    "                # Definicion del umbral para los valores atipicos (se establece x veces la desviacion estandar)\n",
    "                threshold = t_times * group_std\n",
    "\n",
    "                # Identificacion de los valores atipicos\n",
    "                outliers = group[(group[col] - group_mean).abs() > threshold]\n",
    "\n",
    "                data.loc[data['ts'].isin(outliers['ts']), col] = group_median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de las lecturas del pluviómetro instalado con datos de API meteorológica\n",
    "En \"_/data/raw_data/_\" se encuentra el fichero \"_estacion_aranda.json_\" que contiene los datos meteorológicos de la estación meteorológica de la AEMET en Aranda de Duero desde el 20/07/2021 hasta el 22/02/2023.\n",
    "\n",
    "Los datos en este fichero JSON tienen la siguiente forma:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fecha\" : \"2021-07-20\",\n",
    "  \"indicativo\" : \"2117D\",\n",
    "  \"nombre\" : \"ARANDA DE DUERO\",\n",
    "  \"provincia\" : \"BURGOS\",\n",
    "  \"altitud\" : \"790\",\n",
    "  \"tmed\" : \"24,2\",\n",
    "  \"prec\" : \"0,0\",\n",
    "  \"tmin\" : \"13,7\",\n",
    "  \"horatmin\" : \"05:20\",\n",
    "  \"tmax\" : \"34,8\",\n",
    "  \"horatmax\" : \"14:00\",\n",
    "  \"dir\" : \"21\",\n",
    "  \"velmedia\" : \"1,4\",\n",
    "  \"racha\" : \"8,1\",\n",
    "  \"horaracha\" : \"15:00\",\n",
    "  \"sol\" : \"7,9\",\n",
    "  \"presMax\" : \"927,6\",\n",
    "  \"horaPresMax\" : \"Varias\",\n",
    "  \"presMin\" : \"923,9\",\n",
    "  \"horaPresMin\" : \"14\"\n",
    "}\n",
    "```\n",
    "\n",
    "De los datos reflejados se emplearán \"_fecha_\" y \"_prec_\", este úiltimo parámetro medido en mm (se convertirá posteriormente a litros/m<sup>2</sup>).\n",
    "Al cargar los datos de la API se realiza una transformación a otro formato, de forma que se instancia un diccionario con \"_fecha_\" como clave y los la estructura reflejada anteriormente como valor, de esta forma el acceso a la información de las precipitaciones se agilizará y no será necesario recorrer el vector del fichero JSON por cada uno de los grupos de datos.\n",
    "\n",
    "Al igual que en los datos de los sensores, los datos del pluviómetro se agruparán por frecuencias de días, de manera que se calculará la media del grupo. Si esta difiere en 0,5 litros/m<sup>2</sup> con los datos de la API, se establecen las precipitaciones de ese día a las medias de la estación meteorológica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluviuometer_comparison(data):\n",
    "    with open(PLUVIOMETER_FILES_DIRECTORY + \"estacion_aranda.json\") as f:\n",
    "        # Transformacion del json original (creado como una lista de diccionarios)\n",
    "        # en un diccionario con las fechas como claves y el resto de campos como valores\n",
    "        original_api_data = json.load(f)\n",
    "        api_data = dict()\n",
    "        for sample in original_api_data:\n",
    "            api_data.update({sample[\"fecha\"]: sample})\n",
    "\n",
    "        # Ventana de datos agrupados por dia\n",
    "        rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "        for day, group in rolling_win:\n",
    "            # Calculo de la media de las precipitaciones del grupo (precipitaciones\n",
    "            # diarias en litros/m^2)\n",
    "            group_mean = group[\"pluv_deltaMM\"].mean()\n",
    "            \n",
    "            # Obtenemos el dia del grupo como cadena de texto para poder\n",
    "            # emplearlo como clave en el diccionario y asi poder comparar las \n",
    "            # precipitaciones\n",
    "            day = str(day.date())\n",
    "\n",
    "            # Si las precipitaciones medias difieren en 0,5 litros/m^2 establecemos\n",
    "            # las precipitaciones de ese dia a las de la API.\n",
    "            if abs(group_mean - float(api_data[day][\"prec\"].replace(\",\", \".\")) * 0.1) > 0.5:\n",
    "                data.loc[data['ts'].isin(group['ts']), \"pluv_deltaMM\"] = float(api_data[day][\"prec\"].replace(\",\", \".\"))\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos del pluviómetro\n",
    "Al igual que en el conjunto de datos de los sensores, se empleará Pandas para la carga al programa de las muestras realizadas y se realizará la conversión de las marcas de tiempo.\n",
    "\n",
    "En cuanto a los valores desconocidos, se seguirá el mismo procedimiento que en la sección anterior (en esta ocasión la cantidad de valores es sutancialmente menor).\n",
    "En función de los datos obtenidos en la API meteorológica se realizará la sustitución de las muestras correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluviometer_data_processing(pluviometer_files):\n",
    "    for file in pluviometer_files:\n",
    "        dataset = pd.read_csv(PLUVIOMETER_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminar campos sin valores\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "        dataset = dataset.drop(['fecha', 'bateria'], axis=1)\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 2)\n",
    "\n",
    "        # Sustitucion de los datos del pluviometro en funcion de las lluvias diarias\n",
    "        pluviuometer_comparison(dataset)\n",
    "\n",
    "        dataset['ts'] = pd.to_numeric(dataset['ts'])\n",
    "\n",
    "        dataset.to_csv(CLASSIFIED_PLUVIOMETER_PATH + file, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversión del conjunto de datos en datos diarios\n",
    "\n",
    "Convertimos el conjunto de datos original con muestras tomadas cada 5 minutos en datos medios diarios, para de esta forma obtener las tendencias de las diferentes variables empleando el filtro de \n",
    "Hodrick-Prescott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_sensor_data(data, lamb):\n",
    "    # Ventana de datos agrupados por dia\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "\n",
    "    # Bucle para determinar las medias diarias de las diferentes variables\n",
    "    series = []\n",
    "    for day, group in rolling_win:\n",
    "        # Si el grupo tiene datos, calculamos su media diaria de cada variable\n",
    "        if(group.shape[0] > 0):\n",
    "            data_day = group[INPUT_COLS].mean()\n",
    "            \n",
    "            data_day['ts'] = day\n",
    "\n",
    "            series.append(data_day)\n",
    "    dataset = pd.DataFrame(data=series)\n",
    "\n",
    "    # Bucle para determinar las tendencias de cada variable mediante filtro de Hodrick-Prescott\n",
    "    for col in INPUT_COLS:\n",
    "        _, trend = sm.tsa.filters.hpfilter(dataset[col], lamb=lamb)\n",
    "        dataset[col] = trend\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_pluviometer_data(data):\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "\n",
    "    # Bucle para determinar las medias diarias de las diferentes variables\n",
    "    series = []\n",
    "    for day, group in rolling_win:\n",
    "        # Si el grupo tiene datos, calculamos su media diaria de cada variable\n",
    "        if(group.shape[0] > 0):\n",
    "            data_day = group[INPUT_COLS].mean()\n",
    "            \n",
    "            data_day['ts'] = day\n",
    "\n",
    "            series.append(data_day)\n",
    "    dataset = pd.DataFrame(data=series)\n",
    "\n",
    "    # Bucle para determinar las tendencias de cada variable mediante filtro de Hodrick-Prescott\n",
    "    for col in INPUT_COLS:\n",
    "        _, trend = sm.tsa.filters.hpfilter(dataset[col], lamb=100)\n",
    "        dataset[col] = trend\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos de los sensores\n",
    "Para cada uno de los ficheros de los sensores realizaremos varias operaciones. En primer lugar, empleando Pandas cargaremos el \"_DataFrame_\" en el programa y tras esto eliminaremos las filas con valores marcados como inválidos (procesamiento preliminar realizado de forma visual).\n",
    "\n",
    "Para agrupar los datos en frecuencias de días es necesario realizar una conversión \"_cast_\" del tipo de dato de la columna \"_ts_\" del conjunto de datos.\n",
    "Las columnas \"_fecha_\", \"_h_C_\" y \"_h_L_\" son redundantes, en el primer caso puesto que la información correspondiente se encuentra en forma de _Linux Epochs_ en la columna \"_ts_\" de la que previamente se ha realizado una transformación y que actúa como índice temporal del conjunto de datos. En el caso de las otras dos columnas, se tratan de datos relacionados con la humedad del suelo sin calibrar a diferentes profundidades, encontrandose estas calibradas en \"_h_C_cal_\" y \"_h_L_cal_\" respectivamente.\n",
    "\n",
    "Posteriormente se eliminarán los valores desconocidos (\"_Missing Data_\") debido a la gran cantidad de datos disponibles (en las visualizaciones previas se comprobó visualmente los posibles impactos en el tamaño del conjunto de datos). Finalmente se realizará la detección de valores extremos (\"_outliers_\") con diferentes umbrales y se reconvertirá las marcas de tiempo de nuevo a _Linux Epochs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_data_processing(sensor_files):\n",
    "    for file in sensor_files:\n",
    "        dataset = pd.read_csv(SENSOR_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminamos los datos marcados como invalidos\n",
    "        dataset = dataset[dataset['validez'] != 0]\n",
    "\n",
    "        # Transformamos los timestamps en datetimes, para posteriormente poder\n",
    "        # agrupar los datos por dia.\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "\n",
    "        # Eliminamos las columnas de fecha, h_C, h_L (por redundancia) y la columna\n",
    "        # de la bateria (no es esencial en la regresion).\n",
    "        # Por otro, lado deshechamos la columna de validez (no es necesaria)\n",
    "        dataset = dataset.drop(['fecha', 'h_C', 'h_L', 'bateria', 'validez'], axis=1)\n",
    "\n",
    "        # Eliminar nulos (data missing)\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 1.2 if file == \"sensor6.csv\" else 2)\n",
    "\n",
    "        # Datos por día\n",
    "        dataset = day_sensor_data(dataset, 100)\n",
    "\n",
    "        dataset['ts'] = pd.to_numeric(dataset['ts'])\n",
    "        \n",
    "        dataset.to_csv(CLASSIFIED_SENSOR_PATH + file, index = False)\n",
    "\n",
    "        print(f\"Sensor: {file} procesado\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución del procesamiento de datos\n",
    "Se obtendrá los ficheros de los datos sin procesar almacenados en los directorios expuestos con anterioridad, invocando a las funciones de procesamiento correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: sensor1.csv procesado\n",
      "Sensor: sensor2.csv procesado\n",
      "Sensor: sensor3.csv procesado\n",
      "Sensor: sensor4.csv procesado\n",
      "Sensor: sensor5.csv procesado\n",
      "Sensor: sensor6.csv procesado\n",
      "Sensor: sensor7.csv procesado\n",
      "Sensor: sensor8.csv procesado\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sensor_files = [file for file in os.listdir(SENSOR_FILES_DIRECTORY) if \".csv\" in file]\n",
    "    pluviometer_files = [file for file in os.listdir(PLUVIOMETER_FILES_DIRECTORY) if \".csv\" in file]\n",
    "\n",
    "    sensor_data_processing(sensor_files)\n",
    "    pluviometer_data_processing(pluviometer_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f22e608a9410c9a00adbb49e3cb6a72010c497ae6b30c9496ff58de178a89c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
