{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos para poder ser empleados por la _RNA_\n",
    "\n",
    "Los datos empleados en el proyecto se encuentran en el directorio _\"/data/\"_, de forma que se dividen en puros (raw) y clasificados (classified) e integrados (integrated):\n",
    "\n",
    "```\n",
    ".\n",
    "|-data/\n",
    "   |- raw_data/ -> ficheros de datos sin procesar (procesado preliminar).\n",
    "   |     |- sensores/ -> ficheros correspondientes a los datos de los sensores.\n",
    "   |     |- pluviometro/ -> ficheros correspondientes a los datos del pluviómetro.\n",
    "   |- classified_data/ -> ficheros de datos procesados.\n",
    "   |     |- sensores/ -> ficheros correspondientes a los datos de los sensores.\n",
    "   |     |- pluviometro/ -> ficheros correspondientes a los datos del pluviómetro.\n",
    "   |- integrated_data/ -> fichero con todos los datos integrados. \n",
    "```\n",
    "\n",
    "Importamos las librerías para el procesamiento de los datos.\n",
    "\n",
    "Para la ETL (extracción, transformación y carga) emplearemos Pandas.\n",
    "\n",
    "Por otro lado, en el caso de los datos del pluviómetro para comprobar los valores anómalos emplearemos la librería nativa json utilizando datos de la estación meteorológica de la AEMET situada en Aranda de Duero, ya que es la que más próxima se encuentra al viñedo (situado en el municipio de Anguix, Burgos).\n",
    "El contenido de los directorios _\"/data/raw_data/sensores\"_ y _\"/data/raw_data/pluviometro\"_  será listado mediante el módulo nativo de Python _\"os\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las constantes para establecer la dirección en la que se encuentran los ficheros de datos y el directorio en el que se deberá almacenar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de directorios y variables globales\n",
    "from env_paths import *\n",
    "from global_variables import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección los valores atípicos\n",
    "La _\"detección de valores atípicos\"_ permite detectar los valores anómalos (observaciones numéricamente diferentes al resto) en los datos debidos a diversos factores como errores humanos, mecánicos extremos de lecturas genuinas o por reemplazos de valores perdidos (_missing data_).\n",
    "\n",
    "En este caso se empleará el análisis de _outliers_ por medio del rango intercuartílico mediante una ventana móvil establecida agrupando las muestras correspondientes al mismo día, de forma que por cada grupo se calcula su media, desviación típica y mediana.\n",
    "Se establece un umbral (_threshold_) de X veces la desviación estándar, siendo aquellas muestras cuya diferencia con la media del grupo que se encuentren por encima de este umbral modificadas estableciendo la mediana de la variable correspondiente en el análisis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(data, iqr):\n",
    "    # Ventana de datos agrupados por dia\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "\n",
    "    for _, group in rolling_win:\n",
    "        # Para cada columna realizamos el analisis por deteccion de valores atipicos\n",
    "        for col in data.columns:\n",
    "            if(col != 'ts'):\n",
    "                # Calculo de la media y la desviacion tipica del grupo\n",
    "                group_mean = group[col].mean()\n",
    "                group_std = group[col].std()\n",
    "                group_median = group[col].median()\n",
    "\n",
    "                # Definicion del umbral para los valores atipicos (se establece x veces la desviacion estandar)\n",
    "                threshold = iqr * group_std\n",
    "\n",
    "                # Identificacion de los valores atipicos\n",
    "                outliers = group[(group[col] - group_mean).abs() > threshold]\n",
    "\n",
    "                data.loc[data['ts'].isin(outliers['ts']), col] = group_median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de las lecturas del pluviómetro instalado con datos de API meteorológica\n",
    "En \"_/data/raw_data/_\" se encuentra el fichero \"_estacion_aranda.json_\" que contiene los datos meteorológicos de la estación meteorológica de la AEMET en Aranda de Duero desde el 20/07/2021 hasta el 22/02/2023.\n",
    "\n",
    "Los datos en este fichero JSON tienen la siguiente forma:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fecha\" : \"2021-07-20\",\n",
    "  \"indicativo\" : \"2117D\",\n",
    "  \"nombre\" : \"ARANDA DE DUERO\",\n",
    "  \"provincia\" : \"BURGOS\",\n",
    "  \"altitud\" : \"790\",\n",
    "  \"tmed\" : \"24,2\",\n",
    "  \"prec\" : \"0,0\",\n",
    "  \"tmin\" : \"13,7\",\n",
    "  \"horatmin\" : \"05:20\",\n",
    "  \"tmax\" : \"34,8\",\n",
    "  \"horatmax\" : \"14:00\",\n",
    "  \"dir\" : \"21\",\n",
    "  \"velmedia\" : \"1,4\",\n",
    "  \"racha\" : \"8,1\",\n",
    "  \"horaracha\" : \"15:00\",\n",
    "  \"sol\" : \"7,9\",\n",
    "  \"presMax\" : \"927,6\",\n",
    "  \"horaPresMax\" : \"Varias\",\n",
    "  \"presMin\" : \"923,9\",\n",
    "  \"horaPresMin\" : \"14\"\n",
    "}\n",
    "```\n",
    "\n",
    "De los datos reflejados se emplearán \"_fecha_\" y \"_prec_\", este úiltimo parámetro medido en mm (se convertirá posteriormente a litros/m<sup>2</sup>).\n",
    "Al cargar los datos de la API se realiza una transformación a otro formato, de forma que se instancia un diccionario con \"_fecha_\" como clave y los la estructura reflejada anteriormente como valor, de esta forma el acceso a la información de las precipitaciones se agilizará y no será necesario recorrer el vector del fichero JSON por cada uno de los grupos de datos.\n",
    "\n",
    "Al igual que en los datos de los sensores, los datos del pluviómetro se agruparán por frecuencias de días, de manera que se calculará la media del grupo. Si esta difiere en 0,5 litros/m<sup>2</sup> con los datos de la API, se establecen las precipitaciones de ese día a las medias de la estación meteorológica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluviuometer_comparison(data):\n",
    "    with open(PLUVIOMETER_FILES_DIRECTORY + \"estacion_aranda.json\") as f:\n",
    "        # Transformacion del json original (creado como una lista de diccionarios)\n",
    "        # en un diccionario con las fechas como claves y el resto de campos como valores\n",
    "        original_api_data = json.load(f)\n",
    "        api_data = dict()\n",
    "        for sample in original_api_data:\n",
    "            api_data.update({sample[\"fecha\"]: sample})\n",
    "\n",
    "        # Ventana de datos agrupados por dia\n",
    "        rolling_win = data.groupby(pd.Grouper(freq=\"1H\", key=\"ts\"))\n",
    "        for day, group in rolling_win:\n",
    "            # Calculo de la media de las precipitaciones del grupo (precipitaciones\n",
    "            # diarias en litros/m^2)\n",
    "            group_mean = group[\"pluv_deltaMM\"].mean()\n",
    "            \n",
    "            # Obtenemos el dia del grupo como cadena de texto para poder\n",
    "            # emplearlo como clave en el diccionario y asi poder comparar las \n",
    "            # precipitaciones\n",
    "            day = str(day.date())\n",
    "\n",
    "            # Si las precipitaciones medias difieren en 0,5 litros/m^2 establecemos\n",
    "            # las precipitaciones de ese dia a las de la API.\n",
    "            if abs(group_mean - float(api_data[day][\"prec\"].replace(\",\", \".\")) * 0.1) > 0.5:\n",
    "                data.loc[data['ts'].isin(group['ts']), \"pluv_deltaMM\"] = float(api_data[day][\"prec\"].replace(\",\", \".\"))\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversión del conjunto de datos en datos de tendencia\n",
    "\n",
    "Convertimos el conjunto de datos original con muestras tomadas cada 5 minutos en datos medios, para de esta forma obtener las tendencias de las diferentes variables empleando el filtro de \n",
    "Hodrick-Prescott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, freq = \"1H\", lamb = 1600, filter_cols = [], cols = []):\n",
    "    # Ventana de datos agrupados por frecuencia\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=freq, key=\"ts\"))\n",
    "\n",
    "    # Bucle para determinar las medias temporales de las diferentes variables teniendo en cuenta los posibles saltos\n",
    "    series = []\n",
    "    win_serie = []\n",
    "    for time, group in rolling_win:\n",
    "        # Si el grupo tiene datos, calculamos su media diaria de cada variable\n",
    "        if group.shape[0] > 0:\n",
    "            data_mean = group[filter_cols].mean()\n",
    "            data_mean['ts'] = time\n",
    "            win_serie.append(data_mean)\n",
    "        else:\n",
    "            # Si no hay datos es porque se ha producido un salto temporal, guardamos la serie completa\n",
    "            # y la limpiamos para comenzar la siguiente ventana de datos presentes.\n",
    "            if len(win_serie) > 0:\n",
    "                series.append(win_serie)\n",
    "                win_serie = []\n",
    "    series.append(win_serie)\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=cols)\n",
    "    # Bucle para determinar las tendencias de cada variable mediante filtro de Hodrick-Prescott\n",
    "    for serie in series:\n",
    "        serie_df = pd.DataFrame(data=serie, columns=cols)    \n",
    "        if serie_df.shape[0] > 1:\n",
    "            for col in filter_cols:\n",
    "                _, trend = sm.tsa.filters.hpfilter(serie_df[col], lamb=lamb)\n",
    "                serie_df[col] = trend\n",
    "\n",
    "        dataset = pd.concat([dataset, serie_df])\n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos del pluviómetro\n",
    "Al igual que en el conjunto de datos de los sensores, se empleará Pandas para la carga al programa de las muestras realizadas y se realizará la conversión de las marcas de tiempo.\n",
    "\n",
    "En cuanto a los valores desconocidos, se seguirá el mismo procedimiento que en la sección anterior (en esta ocasión la cantidad de valores es sutancialmente menor).\n",
    "En función de los datos obtenidos en la API meteorológica se realizará la sustitución de las muestras correspondientes.\n",
    "\n",
    "Para obtener los datos por horas y posteriormente poder unirlos a los sensores calculamos las medias de las precipitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluviometer_data_processing(pluviometer_files):\n",
    "    for file in pluviometer_files:\n",
    "        dataset = pd.read_csv(PLUVIOMETER_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminar campos sin valores\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "\n",
    "        # Eliminar columnas irrelevantes o con datos que expresan el mismo dominio del problema\n",
    "        dataset = dataset.drop(['fecha', 'bateria', 'pluv', 'pluv_delta'], axis=1)\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 1.5)\n",
    "\n",
    "        # Sustitucion de los datos del pluviometro en funcion de las lluvias diarias\n",
    "        pluviuometer_comparison(dataset)\n",
    "\n",
    "        # Datos medios por hora\n",
    "        cols = ['ts', 'pluv_deltaMM']\n",
    "        day_data = pd.DataFrame(columns=cols)\n",
    "        rolling_win = dataset.groupby(pd.Grouper(freq=\"1H\", key=\"ts\"))\n",
    "        for time, group in rolling_win:\n",
    "            # Si el grupo tiene datos, calculamos la media de las precitipaciones en la hora\n",
    "            if group.shape[0] > 0:\n",
    "                data_mean = group[['pluv_deltaMM']].mean()\n",
    "                data_mean['ts'] = time\n",
    "                serie_df = pd.DataFrame(data=[data_mean], columns=cols) \n",
    "\n",
    "                day_data = pd.concat([day_data, serie_df])\n",
    "        \n",
    "        day_data['ts'] = pd.to_numeric(day_data['ts'])\n",
    "\n",
    "        day_data.to_csv(CLASSIFIED_PLUVIOMETER_PATH + file, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos de los sensores\n",
    "Para cada uno de los ficheros de los sensores realizaremos varias operaciones. En primer lugar, empleando Pandas cargaremos el \"_DataFrame_\" en el programa y tras esto eliminaremos las filas con valores marcados como inválidos (procesamiento preliminar realizado de forma visual).\n",
    "\n",
    "Para agrupar los datos en frecuencias por horas es necesario realizar una conversión \"_cast_\" del tipo de dato de la columna \"_ts_\" del conjunto de datos.\n",
    "Las columnas \"_fecha_\", \"_h_C_\" y \"_h_L_\" son redundantes, en el primer caso puesto que la información correspondiente se encuentra en forma de _Linux Epochs_ en la columna \"_ts_\" de la que previamente se ha realizado una transformación y que actúa como índice temporal del conjunto de datos. En el caso de las otras dos columnas, se tratan de datos relacionados con la humedad del suelo sin calibrar a diferentes profundidades, encontrandose estas calibradas en \"_h_C_cal_\" y \"_h_L_cal_\" respectivamente.\n",
    "\n",
    "Posteriormente se eliminarán los valores desconocidos (\"_Missing Data_\") debido a la gran cantidad de datos disponibles (en las visualizaciones previas se comprobó visualmente los posibles impactos en el tamaño del conjunto de datos). Finalmente se realizará la detección de valores extremos (\"_outliers_\") con diferentes umbrales y se reconvertirá las marcas de tiempo de nuevo a _Linux Epochs_.\n",
    "\n",
    "Finalmente se obtienen las tendecias de los atributos cada hora con el filtrado de Hodrick-Prescott."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_data_processing(sensor_files):\n",
    "    for file in sensor_files:\n",
    "        dataset = pd.read_csv(SENSOR_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminamos los datos marcados como invalidos\n",
    "        dataset = dataset[dataset['validez'] != 0]\n",
    "\n",
    "        # Transformamos los timestamps en datetimes, para posteriormente poder\n",
    "        # agrupar los datos por dia.\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "\n",
    "        # Eliminamos las columnas de fecha, h_C, h_L (por redundancia) y la columna\n",
    "        # de la bateria (no es esencial en la regresion).\n",
    "        # Por otro, lado deshechamos la columna de validez (no es necesaria)\n",
    "        dataset = dataset.drop(['fecha', 'h_C', 'h_L', 'bateria', 'validez'], axis=1)\n",
    "\n",
    "        # Eliminar nulos (data missing)\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 1.5)\n",
    "\n",
    "        # Datos por hora\n",
    "        dataset = filter_data(dataset, freq=\"1H\", lamb=100, filter_cols=OUTPUT_COLS, cols=dataset.columns.to_list())\n",
    "\n",
    "        dataset['ts'] = pd.to_numeric(dataset['ts'])\n",
    "        \n",
    "        dataset.to_csv(CLASSIFIED_SENSOR_PATH + file, index = False)\n",
    "\n",
    "        print(f\"Sensor: {file} procesado\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución del procesamiento de datos\n",
    "Se obtendrá los ficheros de los datos sin procesar almacenados en los directorios expuestos con anterioridad, invocando a las funciones de procesamiento correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: sensor1.csv procesado\n",
      "Sensor: sensor2.csv procesado\n",
      "Sensor: sensor3.csv procesado\n",
      "Sensor: sensor4.csv procesado\n",
      "Sensor: sensor5.csv procesado\n",
      "Sensor: sensor6.csv procesado\n",
      "Sensor: sensor7.csv procesado\n",
      "Sensor: sensor8.csv procesado\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sensor_files = [file for file in os.listdir(SENSOR_FILES_DIRECTORY) if \".csv\" in file]\n",
    "    pluviometer_files = [file for file in os.listdir(PLUVIOMETER_FILES_DIRECTORY) if \".csv\" in file]\n",
    "\n",
    "    sensor_data_processing(sensor_files)\n",
    "    pluviometer_data_processing(pluviometer_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f22e608a9410c9a00adbb49e3cb6a72010c497ae6b30c9496ff58de178a89c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
