\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

En este apartado se describirán los aspectos más importantes surgidos a partir del desarrollo del proyecto,
estos incluyen las complicaciones encontradas, así como las decisiones tomadas para solucionar todos los
problemas surgidos y sus implicaciones.
Para una especificación más concreta de las situaciones presentadas pueden consultarse los anexos del proyecto correspondientes.

La sección se orientará siguiendo una estructura parecida a la introducida en \textit{Conceptos teóricos}, 
es decir, desarrollando los aspectos relevantes según la fase del proyecto en la que se produjo.

\section{Inicio del Proyecto}
Una de las motivaciones que se tuvo en cuenta para el desarrollo del proyecto fue la cercanía del tema tratado
con el lugar que se desarrolló. Y es que la provincia de Burgos se encuentra en la denominación de
origen ``Ribera del Duero'', una de las regiones que cuenta con algunos de los mejores vinos del mundo \cite{misc:abc2021}.

Otro impulso que llevó a realizar el Trabajo Final de Grado con este tema fue, de igual forma, la importancia 
del vino en la familia, pues el sustento de parte de esta es gracias a la viticultura, además de la historia
e importancia de la cultura del vino en el todo el territorio español.

Tras contactar con los docentes, se proporcionó el conjunto de datos empleado en el proyecto. En estas conversaciones
iniciales se consideró que estos contaban con ciertas características que debían tenerse en cuenta: en primer lugar,
los presentaban valores faltantes debidos a la posible desconexión de los sensores de la red, lo que provocaba
que estos dejaran de enviar datos al servidor. Por otro lado, las labores del campo podían haber afectado a la calidad
de estos por la extracción de las sondas de los sensores de sus posiciones originales.

Además, el pluviómetro instalado en un origen se encontraba anclado de forma débil, lo que propiciaba que el viento 
produjera lecturas erróneas en el movimiento del balancín empleado en el registro de las precipitaciones.

Tras estas reuniones se inició el proceso de la comprensión del negocio, es decir, se comenzó con la definición de los 
objetivos del proyecto y los requisitos. Simplificando lo mencionado en la introducción y los objetivos,
la finalidad era analizar los datos proporcionados (centrándose en la temperatura y humedad) realizando
las visualizaciones pertinentes y encontrar modelos predictivos que intenten anticipar la evolución a corto plazo
empleando diferentes técnicas como las Redes Neuronales Artificiales.

\section{Fase de preparación de los datos}
En la fase de preparación se determinó que los datos proporcionados contaban con diferentes situaciones. En algunos
de los sensores la calidad de estos era bastante adecuada a pesar de contar con valores faltantes, en otros, en cambio,
existían numerosos problemas.

Por un lado, contaban con valores extremos que debían tenerse en cuenta para que los modelos que se desarrollasen no 
encontraran problemas al generalizar, desviándose de ese propósito por el posible ruido introducido.
Para solucionar este problema se empleó el mecanismo de detección de \textit{outliers} mediante el rango intercuartílico,
sustituyendo los datos identificados como extremos con la mediana del día en el que se realizó la muestra.

Por otro lado, algunos de los sensores contaban con valores matemáticamente correctos (dentro de límites lógicos de las 
diferentes variables) pero que contextualmente eran incorrectos. Un ejemplo de esto eran las lecturas artificiales de 
distintas variables como la temperatura cuando el nivel de la batería de los sensores caía por debajo de 3,3V, lo que
provocaba un desbordamiento de la variable en el controlador correspondiente.

Ante esta situación se decidió eliminar aquellas muestras cuyo nivel de batería se encontrara por debajo del mencionado
y por encima de los 4,1V (el máximo). Sin embargo, tras realizar esta operación se observó que ciertas 
muestras continuaban siendo contextualmente incorrectas arrojando datos inverosímiles (bajadas de temperaturas y/o 
humedades extremas en periodos de tiempo cortos) debidas en gran medida, como se confirmó posteriormente, 
a la extracción de las sondas de la situación original, por lo que se acordó con los docentes la eliminación de estos 
valores que continuaban siendo erróneos.

El proceso se realizó a mano teniendo en cuenta las situaciones localizadas con anterioridad y con las nuevas necesidades,
marcando en los diferentes ficheros la validez de la entrada (cabe destacar que la recuperación de estos datos no era viable 
por las condiciones en la que se encontraban, puesto que el lapso temporal era demasiado elevado).

Además de estas condiciones, algunos de los sensores contaban con muestras con discontinuidades inexplicables 
mediante los datos, por lo que se acordó el intento de la recuperación de estos. Para ello, se observaron los muestreos
de los sensores geográficamente más cercanos al afectado, de forma que se pudiera conocer de manera más 
precisa cuál de las series era la que contaba con la lectura correcta y actuar en consecuencia.

A pesar de que se haya podido introducir algún sesgo debido a la modificación de los valores plasmados en los muestreos, 
los datos reales serán más cercanos a los nuevos valores asignados que a las lecturas anteriores, es decir, aún si 
estos fueran incorrectos serían menos incorrectos que anteriormente.

Ante la premura de los plazos del proyecto, realizar de nuevo el muestreo de los datos suficientes para crear un modelo predictivo
eficiente no era viable, por lo que se tuvo que trabajar con la cantidad proporcionada.

\section{Fase de creación de modelos}
Tras realizar una investigación preliminar acerca de los modelos con mayor precisión 
en la regresión de series temporales, se llegó a la conclusión de emplear
redes neuronales recurrentes con memoria como primer objetivo de modelado, 
puesto que sobre datos meteorológicos contaban con un buen rendimiento.

Por ese motivo, se escogió utilizar las redes neuronales con memoria a corto y medio
plazo como son \textit{GRU} y \textit{LSTM}, empleando los mismos hiperparámetros y,
de esta manera, poder realizar una comparativa para tener una idea de cómo se 
ajustan estos modelos a las diferentes funciones objetivo.

Además, otra opción a emplear fue la tradicional \textit{MLP} para poder observar
cómo la linealidad de este modelo (no tiene en cuenta instantes anteriores) impide obtener unos 
resultados aceptables en comparación de sus contrapartes recurrentes.

El primer reto consistía en preparar los datos de entrada a la red neuronal y las 
correspondientes salidas de forma que estos fueran muestras homogéneas
sin saltos temporales que impidieran a los modelos ajustarse adecuadamente.

En primer lugar, en un espacio de 10 minutos se podían encontrar 2 o 3 muestras según fuera 
el \textit{offset} del controlador de muestreos de los sensores.
Además, como en la fase de preparación de los datos se realizaron descartes, se generaron, de esta manera,
saltos temporales que podían afectar a la predicción de la tendencia.

Una solución fue recorrer el conjunto de datos del sensor con una ventana deslizante del tamaño
equivalente a las instancias de entrada (establecido por parámetro) y el dato de salida (se realizó una 
aproximación preliminar realizando una única predicción, que posteriormente se modificó para 
que los modelos fuesen capaces de realizar varias), de esta forma en ventanas en las que 
no había datos suficientes para la entrada se descartaban.

\imagen{window.png}{Esquema del proceso de preparación de datos}{0.5}

Cabe destacar que inicialmente se emplearon todos los datos para entrenamiento y validación, no estableciendo
el conjunto de test, lo que fue modificado más tarde añadiendo el tercer conjunto, permitiendo reducir el sesgo en 
la evaluación del modelo, pues, a diferencia de las instancias de validación, se tratan
de datos que no han sido empleados por el modelo tanto para ser entrenado como validado.

Tras la preparación de los datos de entrada de la red se comenzó a realizar el modelo \textit{GRU}, de forma
que se debían establecer sus capas y las celdas en estas. Posteriormente se realizaron los modelos
\textit{LSTM} y \textit{MLP}.
Para agilizar la construcción y verificar su funcionamiento se estableció inicialmente 3 capas en todos ellos, con
36, 16 y 8 unidades sucesivamente.

A diferencia de los dos primeros, \textit{MLP} requería de una ligera modificación, puesto que no aceptaba secuencias, 
por lo que se estableció una neurona de entrada por cada valor en la serie de tiempo y de igual forma una 
neurona en la capa de salida por cada predicción. 

\section{Fase de evaluación de modelos}
En la fase de evaluación se estableció realizar pruebas sobre los modelos modificando ciertos hiperparámetros como 
el número de capas ocultas, el número de unidades (celdas o neuronas dependiendo del contexto) por capa, el
ratio de aprendizaje y el número de predicciones que se realizan por cada dato de entrada.

De esta forma, los modelos empleados tendrán las especificaciones estándar en el caso de
\textit{GRU} y \textit{LSTM}, modificando la función de activación de las neuronas en las capas ocultas en 
\textit{MLP} por una función \textit{ReLU} (Figura \ref{fig:relu.png}), que aportará la no linealidad al modelo, lo que previsiblemente
mejoraría el rendimiento concreto con los datos empleados.

\imagen{relu.png}{Función ReLU}{0.75}

Se realizarán, entonces, combinaciones con diferentes valores:
\begin{itemize}
    \item Capas: 1, 2 y 3 capas.
    \item Tamaño de capa: 8, 16 y 32 celdas/neuronas.
    \item Ratio de aprendizaje: 0.1, 0.01 y 0.001.
    \item Número de ejemplos de salida: 1, 2 y 3 predicciones.
\end{itemize}

Por otro lado, para agilizar el proceso de las pruebas y el entrenamiento, se acordó emplear alguna 
plataforma que permitiera la ejecución paralelizada, haciendo hincapié en tecnologías como Google Colab 
o CUDA.

En el primer caso, la ejecución no requería de mayor preocupación de cara al usuario, más allá de realizar un
par de configuraciones básicas en los entornos.
En el segundo caso, sin embargo, se optó por emplear Docker para crear los contenedores proporcionados por
TensorFlow para crear el entorno que permitiera la ejecución de los entrenamientos de forma paralela, siendo, de
esta manera, una solución portable y fácil de instalar en cualquier máquina.

Los resultados finales se encuentran detallados en los anexos del proyecto, siendo la siguiente 
información un resumen de las pruebas realizadas.

La Figura \ref{fig:Lr_GRU.png} y \ref{fig:Lr_GRU_ampl.png} muestra una comparativa de los resultados 
promedio obtenidos con diferentes valores de ratio de aprendizaje (0.1, 0.01 y 0.001).
En ella se puede observar que se obtienen mejores resultados por término medio con un ratio de
aprendizaje de 0.01, siendo los modelos de 1 predicción por entrada los que generalmente obtienen errores menores.
\imagen{Lr_GRU.png}{Comparativa de ratio de aprendizaje para GRU}{0.9}
\imagen{Lr_GRU_ampl.png}{Comparativa de ratio de aprendizaje para GRU (0.01 y 0.001)}{0.9}

En la Figura \ref{fig:Lr_LSTM.png} se presentan, de igual forma, los resultados de los modelos 
obtenidos con la arquitectura \textit{LSTM}, dónde puede observarse que para 0.01 de ratio de aprendizaje 
se obtienen los modelos con mayor precisión por término medio con 1 y 2 predicciones por entrada. Siendo, 
de esta forma, la configuración de 1 predicción la que menores errores arroje en promedio en comparación 
con el resto de parámetros (Figura \ref{fig:Lr_LSTM_ampl.png}).
\imagen{Lr_LSTM.png}{Comparativa de ratio de aprendizaje para GRU}{0.9}
\imagen{Lr_LSTM_ampl.png}{Comparativa de ratio de aprendizaje para LSTM (0.01)}{0.9}

Por otro lado, la Figura \ref{fig:Lr_MLP.png} puede observarse que el ratio de aprendizaje tiene una 
aparente mayor influencia sobre los resultados, siendo la configuración de 0.001 la que menores errores 
por término medio proporcionan.
\imagen{Lr_MLP.png}{Comparativa de ratio de aprendizaje para LSTM (0.01)}{0.9}

De este modo, se puede realizar también una comparativa de las combinaciones obtenidas de forma 
individual, puesto que aunque en término medio los modelos \textit{LSTM} parecen obtener resultados más
precisos, los resultados con los errores cuadráticos medios más bajos se han obtenido con arquitecturas
\textit{GRU}.

La Tabla \ref{tabla:Tabla comparativa de modelos} muestra los menores errores cuadráticos medios promedio 
(de las 5 ejecuciones por cada una de las combinaciones) junto con el modelo al que se corresponde el dato.
\tablaSmallSinColores{Tabla comparativa de modelos}{l l l l}{Tabla comparativa de modelos}
{Capas & Tamaño capa & LR & Modelo con menor error \\}
{
    1 & 8 & 0.001 & GRU (1.72183E-05)\\
    1 & 8 & 0.01 & GRU (3.37952E-06)\\
    1 & 8 & 0.1 & LSTM (3.9049E-06)\\
    1 & 16 & 0.001 & GRU (9.1907E-06)\\
    1 & 16 & 0.01 & GRU (9.55771E-07)\\
    1 & 16 & 0.1 & LSTM (0.00138)\\
    1 & 32 & 0.001 & GRU (4.50204E-06)\\
    1 & 32 & 0.01 & GRU (\textbf{5.96471E-07})\\
    1 & 32 & 0.1 & LSTM (9.51891E-07)\\

    % 2 capas ocultas
    2 & 8 & 0.001 & GRU (1.56769E-05)\\
    2 & 8 & 0.01 & GRU (1.65277E-06)\\
    2 & 8 & 0.1 & LSTM (6.85972E-06)\\
    2 & 16 & 0.001 & GRU (5.07987E-06)\\
    2 & 16 & 0.01 & GRU (\textbf{5.66822E-07})\\
    2 & 16 & 0.1 & LSTM (6.65626E-06)\\
    2 & 32 & 0.001 & GRU (2.18717E-06)\\
    2 & 32 & 0.01 & GRU (\textbf{2.69205E-07})\\
    2 & 32 & 0.1 & LSTM (4.20038E-06)\\

    % 3 capas ocultas
    3 & 8 & 0.001 & LSTM (4.58400E-06)\\
    3 & 8 & 0.01 & GRU (1.42658E-06)\\
    3 & 8 & 0.1 & LSTM (5.90159E-06)\\
    3 & 16 & 0.001 & GRU (5.18860E-06)\\
    3 & 16 & 0.01 & GRU (\textbf{4.66034E-07})\\
    3 & 16 & 0.1 & LSTM (1.02880E-05)\\
    3 & 32 & 0.001 & GRU (0.01457)\\
    3 & 32 & 0.01 & LSTM (6.54021E-07)\\
    3 & 32 & 0.1 & LSTM (0.00491)\\
}

\newpage

Como puede observarse en la Figura \ref{fig:Freq_mejor.png}, los modelos con menor error para las 
combinaciones probadas se obtendrán en mayor frecuencia con arquitecturas \textit{GRU}.
\imagen{Freq_mejor.png}{Comparativa de frecuencias de menor error en combinaciones.}{0.9}
