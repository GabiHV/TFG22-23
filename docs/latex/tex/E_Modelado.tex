\apendice{Modelado}

\section{Introducción}
En la fase del modelado, como su nombre indica, se crearán los modelos, de forma que al ser
este proceso (\textit{KDD}) iterativo, se podrá considerar con la etapa anterior, pues existen
herramientas que aúnan estos procedimientos.

En este caso, se toman por separado, teniendo en cuenta que los resultados en una
fase afectan de forma directa al resto.

Se estableció el desarrollo de tres redes neuronales diferentes, \textit{MLP}, 
\textit{LSTM} y \textit{GRU}, tratándose las dos últimas de modelos recurrentes.

En esta sección se explicará la transformación del conjunto de datos para poder
realizar el entrenamiento y las respectivas predicciones, así como
las diferentes características de cada uno de los modelos.

\section{Transformación del conjunto de datos}
El primer problema que es necesario afrontar es que estos deberán aceptar varias
entradas de ejemplares, es decir, se necesitarán \(n\) muestras sucesivas para realizar la
regresión.

Por otro lado, también deben ser capaces de realizar \(m\) predicciones, de forma que 
es preciso crear modelos con \(n\) muestras de entrada y \(m\) salidas diferentes.

El segundo problema deriva de la falta de datos en instantes concretos por el procesamiento
realizado previamente, tanto por la supresión de muestras con valores erróneos, como
la de ejemplares con valores faltantes.

Por tanto, se necesita recorrer los datos en busca de los saltos temporales, para obtener
los subconjuntos completos, y de esta manera, asegurar que dentro de éstos estas situaciones
no se producen.

\imagen{window.png}{Selección de la serie de tiempo}{0.75}

En la Figura \ref{fig:window.png} puede observarse el proceso esquematizado.
De esta forma, cada uno de los subconjuntos que no tienen saltos, se recorrerá 
mediante ventanas deslizantes formando los datos de entrada (\textit{inputs}) a los 
modelos y las salidas (\textit{targets}).

Cabe destacar que tanto los subconjuntos como las ventanas deberán contener muestras 
suficientes, es decir, \(n + m\) ejemplos.

\newpage

\section{GRU}
En el caso de \textit{GRU}, las celdas de las que se encuentra compuesto el modelo 
emplearán como funciones de activación la tangente hiperbólica y funciones sigmoides, 
es decir, se emplearán los modelos por defecto como puede observarse en la Figura 
\ref{fig:gru_cell.png}.

\imagen{gru_cell.png}{Representación celda GRU. Extraído de \cite{wiki:gru2022}}{0.70}

\section{LSTM}
Al igual que en \textit{GRU}, \textit{LSTM} empleará funciones sigmoides y la 
tangente hiperbólica como funciones de activación en las compuertas de las celdas (Figura \ref{fig:LSTM_cell.png}),
empleando, de igual manera, modelos por defecto.

\imagen{LSTM_cell.png}{Representación celda LSTM. Extraído de \cite{wiki:lstm2022}}{0.70}

A diferencia de \textit{GRU}, \textit{LSTM} cuenta con 3 puertas diferentes en cada celda, 
la compuerta de olvido (\textit{Forget Gate}), de entrada (\textit{Input Gate}), 
de salida (\textit{Output Gate}) y un estado oculto (\textit{Cell Satate}), mientras que 
el primer modelo únicamente dispone de dos (\textit{Reset Gate} y \textit{Update Gate}).

De esta forma, en los modelos \textit{LSTM}, se necesitarán realizar más operaciones
que en \textit{GRU}, además de almacenar más información por celda, 
de manera que el tiempo de entrenamiento será generalmente mayor.

\section{MLP}
El perceptrón multicapa, a diferencia de los otros dos modelos, no acepta
la entrada de secuencias variables, por lo que se debe establecer una neurona de entrada
por cada uno de los atributos, es decir, la red contendrá
\(a * n\) neuronas de entrada, siendo \(a\) el número de características de los ejemplos y
\(n\) el número de muestras de entrada.

De forma similar, contará con \(a * m\) número de neuronas en la capa de salida, siendo
\(m\) el número de ejemplares de salida.

\imagen{mlp_anexos.png}{Diagrama de entradas y salidas MLP}{0.75}

Se establece una función \textit{ReLU} (Figura \ref{fig:relu.png}) como función
de activación en las neuronas del modelo.

\imagen{relu.png}{Función ReLU}{0.70}

A pesar de que \textit{MLP} no acepta entradas secuenciales y que por tanto requiere de
tantas neuronas de entrada como número de valores hay en la secuencia de entrada,
el tiempo de entrenamiento de estos modelos será menor generalmente que el de los
recurrentes, puesto que se realizan menos operaciones por término medio,
además de necesitar de una cantidad de memoria menor para almacenar los pesos sinápticos.
