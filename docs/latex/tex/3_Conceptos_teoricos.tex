\capitulo{3}{Conceptos teóricos}
El desarrollo del proyecto cuenta con diferentes fases siguiendo el proceso de \textit{descubrimiento de conocimiento
en bases de datos, KDD}, compuesto por la comprensión del negocio, la comprensión de los datos, la preparación de datos,
el modelado, la evaluación del modelo y el despliegue del producto software \cite{book:witten2017}.

\imagen{data_mining_process.jpg}{\textit{Proceso KDD}. Extraído de \cite{book:witten2017}}{0.5}

En la fase de comprensión del negocio se analizan los objetivos y requisitos del proyecto. 
En este caso quedan bien marcados en la descripción del proyecto y la introducción, así como en la sección de los objetivos.

En la etapa de comprensión de los datos se crea el conjunto de datos inicial y se comprueba si este es adecuado,
de forma que si se determina que no lo es, se deberá continuar con la recopilación.
En el caso que nos atañe como estamos sujetos a los plazos del curso, la recopilación de más datos puede complicarse.

En la preparación de los datos se realiza el pre-procesamiento de estos, de forma que puedan ser empleados en el modelado.

En la fase de modelado, como su propio nombre indica, se crean los modelos, lo que se encuentra relacionado estrechamente con
la fase de preparación, puesto que algunas herramientas de pre-procesamiento incluyen un modelo interno 
de los datos para transformarlos \cite{book:witten2017}.

En la fase de la evaluación se estima el rendimiento del modelo, reconsiderándose en su caso los objetivos, de forma 
que si los modelos son poco efectivos se vuelve a la primera fase, ya que se trata de
un proceso iterativo.

No se contempla realizar un despliegue de los modelos resultantes en ninguna aplicación, puesto que se sale del alcance del proyecto,
sin embargo, se obtendrán resultados que pueden ser empleados en posteriores desarrollos de diferentes
índoles.

En esta sección se presentarán los conceptos teóricos relevantes en cada etapa para facilitar su comprensión.

\section{Pre-procesamiento de datos}
En el pre-procesamiento de datos se pretende realizar la integración y limpieza de estos, de forma que disminuyan los posibles problemas
de calidad que puedan surgir en los diferentes sistemas de información.

Como norma general el \textbf{proceso de integración} debe ser realizado durante la fase de recopilación de los datos.
La \textbf{limpieza} permite la detección y corrección de los problemas no resueltos en la fase anterior como los valores anómalos (\textit{Outliers}) o faltantes 
\cite{book:hernandez2004}.  

En este caso, los datos si bien no se encontrarán integrados de forma completa, estarán en formatos compatibles (tanto en lo referente
a los nombres de los atributos como las dimensiones de los valores), de forma que no es necesario
realizar un proceso exhaustivo, simplificando las operaciones a añadir exclusivamente una clase identificativa de los sensores concretos.

Tras la integración de las diferentes fuentes (e.g. bases de datos), se puede realizar un resumen de los atributos, en la que
se mostrarán las características generales de estos como medias, mínimos, máximos y valores posibles. 
En esta tabla podríamos obtener información trascendental para proceso de análisis, sobre todo para atributos categóricos.
En el caso de atributos numéricos un mecanismo visual que es especialmente útil es la gráfica de dispersión \cite{book:hernandez2004}, que es la técnica de visualización
de datos que se empleará mayoritariamente durante el desarrollo del proyecto.

En el conjunto de datos podemos encontrar \textbf{valores faltantes}, que pueden ser reemplazados por diferentes razones. Una de ellas es que
el modelo que empleemos puede no tratar bien estos valores o que utilice un mecanismo de tratamiento que no sea adecuado.
Un problema asociado a su detección es que estos no estén representados como nulos, lo que puede introducir sesgo en el conocimiento
extraído \cite{book:hernandez2004}.

Ante esta situación se puede actuar de diferentes maneras \cite{book:hernandez2004}:
\begin{itemize}
    \item Ignorarlos (ciertos algoritmos son tolerantes a los valores faltantes).
    \item Eliminar el atributo que contiene valores faltantes.
    \item Filtrar las filas: eliminar las filas afectadas, lo que introduce cierto sesgo en muchas ocasiones.
    \item Reemplazar el valor por otro que preserve la media y la varianza del conjunto de datos en caso de atributos numéricos y la moda en atributos
        categóricos.
        Una forma de reemplazar los valores faltantes es la imputación de datos perdidos, que consiste en predecirlos a partir de otros ejemplos. Existen también
        algoritmos que se emplean tradicionalmente para este fin.
    \item Segmentar: se segmentan las filas por los valores disponibles y se obtiene un modelo por cada uno de los segmentos y se combinan.
    \item Modificar la política de calidad de datos y esperar a que los faltantes estén disponibles.
\end{itemize}

Además de las situaciones anteriores, es posible que el conjunto de datos cuente con \textbf{valores erróneos} que 
se deben detectar y tratar.
La detección de estos campos puede realizarse de diferentes maneras.

En el caso que nos atañe se deben buscar los \textbf{valores extremos}, que no significa que sean erróneos, sino que 
estadísticamente se clasifican como anómalos, aunque representen un estado genuino de la realidad.
Con todo y con eso, estos valores pueden suponer un problema para algunos métodos que se basan en el ajuste de pesos
como las redes neuronales.
En otras ocasiones puede haber datos erróneos que caen en la normalidad, por lo que no pueden ser detectados estadísticamente sin 
conocimiento explícito del dominio del problema \cite{book:hernandez2004}.

No detectar estos valores puede resultar en problemas si posteriormente se normalizan los datos, puesto
que la mayoría de datos estarían en un rango muy pequeño \cite{book:hernandez2004}, mientras que unos pocos 
se encontrarán en rangos más grandes y alejados de estos, lo que puede introducir problemas de eficiencia y precisión.

El tratamiento de los datos erróneos o anómalos pueden ser tratados de forma similar a los faltantes \cite{book:hernandez2004}:
\begin{itemize}
    \item Ignorarlos (ciertos algoritmos son robustos a datos anómalos).
    \item Eliminar el atributo que contiene los datos anómalos, por ejemplo, si esta situación se produce
        continuamente (es preferible reemplazarla por otra columna con valores discretos
        estableciendo la corrección o no del valor).
    \item Filtrar las filas: eliminar las filas afectadas, lo que, de nuevo, puede introducir cierto sesgo.
    \item Reemplazar el valor por nulo, por los máximos o mínimos del atributo o por
        las medias (se debe tener en cuenta que los modelos sean capaces de procesar los valores
        nulos, ya que en caso contrario se deberá hacer frente a un nuevo problema en el conjunto
        de datos).
    \item Discretizar: transformar un valor continuo en uno discreto.
\end{itemize}
Los atributos con valores erróneos serán más graves cuando este sea empleado como clase o valor de salida de la predicción \cite{book:hernandez2004},
puesto que afectarán directamente al cálculo del rendimiento del modelo. 

\subsection{Filtro de Hodrick-Prescott}
Además de los mecanismos descritos para realizar el pre-procesamiento de los datos, se emplea el filtro de Hodrick-Prescott, que 
permite extraer los componentes tendenciales y cíclicos de una secuencia temporal, de manera que de los originales se 
puedan utilizar las tendencias para entrenar los diferentes modelos \cite{misc:wikipediaHP}.

Para el cálculo del componente tendencial (\(\tau_t\)), se emplea un valor positivo \(\lambda\), de forma que se resuelve:
\begin{equation}
    \min \sum_{t=1}^T\left(y_t-\tau_t\right)^2+\lambda \sum_{t=2}^{T-1}\left[\left(\tau_{t+1}-\tau_t\right)-\left(\tau_t-\tau_{t-1}\right)\right]^2
\end{equation}

Dónde \(y_t\) se corresponde con:
\begin{equation}
    y_t = \tau_t + c_t
\end{equation}

Siendo de esta forma la componente cíclica calculada cómo:
\begin{equation}
    c_t = y_t - \tau_t
\end{equation}

\section{Modelado}

Para la creación de los modelos emplearemos diferentes tipos de \textit{Redes Neuronales Artificiales}, como 
el perceptrón multicapa (\textit{MLP}), \textit{Gated Recurrent Unit} y \textit{Long Short-Term Memory}.

\subsection{MLP}
El perceptrón multicapa además de una capa de entrada y salida contiene varias capas intermedias
llamadas ocultas (\textit{hidden layers}), denominadas de este modo puesto que los cálculos
internos están ocultos al usuario \cite{book:aggarwal2018}.

En este tipo de redes las señales se propagan inicialmente hacia delante para obtener un resultado, 
de forma que las neuronas de una capa se encuentran interconectadas con las de la capa
siguiente y así sucesivamente en todas ellas (\textit{feed-forward}).

Cada una de estas conexiones cuenta con un peso, que se inicializa generalmente de forma aleatoria. Estos son
empleados para el cálculo de las señales de activación de cada neurona que se utilizarán en la función de activación:

\begin{equation}
    \begin{aligned}
    & \bar{h}_1=\Phi\left(W_1^T \bar{x}\right) \\
    & \bar{h}_{p+1}=\Phi\left(W_{p+1}^T \bar{h}_p\right) \quad \forall p \in\{1 \ldots k-1\}\\
    & \bar{o}=\Phi\left(W_{k+1}^T \bar{h}_k\right)
    \end{aligned}
\end{equation}

Donde \begin{math}\bar{h}_1\end{math} será el resultado de aplicar la función de activación (denotada por \(\Phi\)) en la primera capa oculta (la capa de entrada
actúa de intermedio entre las entradas y la \textit{RNA}), \begin{math}\bar{h}_{p+1}\end{math} serán los resultados de la función de activación
en la capa p-ésima y \begin{math}\bar{o}\end{math} en la capa de salida.

Con respecto a \(W_i\), se trata de la matriz de pesos sinápticos empleados para ajustar los modelos a la función
objetivo, mientras que \(\bar{x}\) se corresponde con los valores de entrada, siendo, de este modo, 
\(\bar{h}_i\) las salidas en las capas i-ésimas.

\imagen{mlp.png}{Perceptrón Multicapa. Extraído de \cite{book:aggarwal2018}}{0.75}

Existen diversas funciones de activación entre las que destacan las sigmoides por ser aplicadas de forma 
elemental \cite{book:aggarwal2018}.

Tras la propagación de las señales de activación hacia adelante, se calculará el gradiente del error empleando el algoritmo
de \textit{Backpropagation}, que utilizará la regla de la cadena del cálculo diferencial para realizar este propósito, de forma 
que los gradientes calculados se emplearán posteriormente para actualizar los pesos de las conexiones sinápticas.

\subsection{Redes Neuronales Recurrentes (\textit{RNN})}
Las redes neuronales convencionales (véase por ejemplo los \textit{MLP}, las redes neuronales de base radial, etc.) están diseñadas para
datos multidimensionales cuyos estados actuales son independientes de los pasados.
Sin embargo, existen datos que contienen dependencias de valores previos como los datos de series temporales, 
biológicos o cadenas de texto \cite{book:aggarwal2018}.

Las redes neuronales recurrentes pueden ser empleadas para este tipo de datos y, además, a diferencia de las redes convencionales recibe y procesa las 
entradas en el orden de llegada y las trata de igual manera que los instantes anteriores.

Además, en lugar de un número variable de entradas, contienen un número variable de celdas (operaciones sobre los datos de entrada, 
para obtener las predicciones), y cada una de estas se corresponde con un 
instante de tiempo, lo que permite a cada una interactuar con otras más cercanas a la salida \cite{book:aggarwal2018}.

Cada una de las celdas realiza operaciones con dos entradas y dos salidas que se calculará en cada instante de tiempo \textit{t} como:
\begin{equation}
    \begin{aligned}
    f: \quad \mathbb{R}^{m+n} & \rightarrow \mathbb{R}^{n+l} \\
    \left(x_t, s_{t-1}\right) & \mapsto\left(y_t, s_t\right)
    \end{aligned}
\end{equation}

Donde \(x_t\) es el vector de entrada de dimensión \textit{m} a la celda en el instante \textit{t}, \(y_{t}\) es el vector de salida de 
dimensión \textit{n} y \(s_{t}\) es el vector de dimensión \textit{l} que representa el estado actual de la red.
Estas celdas compartirán los parámetros internos, por lo que la diferencia entre un instante u otro será las entradas a la celda.

\imagen{rnn.png}{Red Neuronal Recurrente.}{0.25}

En la Figura \ref{fig:rnn.png} puede observarse la representación de una celda de una red recurrente. En
esta puede verse cómo dada una secuencia en el instante \textit{t}, se produce una salida 
\textit{y} que generará una secuencia en el tiempo compartiendo parámetros (pesos sinápticos) como puede apreciarse
en la Figura \ref{fig:rnn_tiempo.png}.

\imagen{rnn_tiempo.png}{Representación de la RNN en el tiempo.}{0.55}

La salida del modelo en un instante de tiempo se calculará, de esta forma cómo \cite{book:rue2019}:
\begin{equation}
    y_t = f(W_{yx}x_t + U_{yy} y_{t-1} + b) 
\end{equation}

Donde \(W_{yx}\) son los pesos sinápticos en el instante actual en la celda, mientras que \(U_{yy}\) se
trata de los pesos de la conexión recurrente, es decir, depende de la salida del instante anterior.
La primera matriz, de esta forma, tendrá un tamaño de \(|x| \cdot |y|\), mientras que la segunda
será de tamaño \(|y| \cdot  |y|\). De manera similar \(|b| = |y|\), tratándose este del vector de sesgo.

A diferencia del modelo expuesto con anterioridad, las redes neuronales recurrentes no pueden aplicar el algoritmo de retropropagación (\textit{Backpropagation})
directamente, puesto que en este caso las celdas de instantes posteriores se encontrarán directamente conectadas con las anteriores.
En este caso se emplea una adaptación conocida como \textit{Backpropagation through time}, que consiste en ``desenrollar'' la red recurrente, convirtiéndola
en una red \textit{feed-forward} y posteriormente aplicar el algoritmo de retropropagación, teniendo en cuenta que dependiendo de los instantes 
que se consideren como salidas de la red el gradiente puede afectar a un mismo parámetro de la red de forma repetida \cite{book:rue2019}.

\subsubsection{LSTM}
Las redes neuronales recurrentes tradicionales pueden encontrar dificultades en capturar y recordar información
a largo plazo debido al problema denominado \textit{desvanecimiento del gradiente}, que ocurre
cuando los gradientes disminuyen excesivamente a medida que se retropropagan a través
del tiempo, produciendo problemas en el aprendizaje a largo plazo.

Para abordar estos problemas, \textit{LSTM} introduce una estructura de memoria adicional denominada
celda de memoria que permite al modelo recordar información durante largos periodos de tiempo. Estos 
modelos emplearán para ello puertas que controlan el flujo de información dentro de estas.

Cada celda en \textit{LSTM} cuenta con tres puertas: la puerta de entrada \textit{Input Gate}, la puerta de salida 
(\textit{Output Gate}) y la puerta de olvido (\textit{Forget Gate}). La primera decide
cuánta información nueva debe agregar a la memoria, la segunda cuánta transmitir como salida y 
la tercera cuánta información antigua se debe de descartar.

En las celdas del modelo se calculan 3 funciones:
\begin{equation}
    f_t = \sigma (W_{f} x_t + U_{f} y_{t-1} + b_f)
\end{equation}
\begin{equation}
    i_t = \sigma (W_{i} x_t + U_{i} y_{t-1} + b_i)
\end{equation}
\begin{equation}    
    o_t = \sigma (W_{o} x_t + U_{o} y_{t-1} + b_o)
\end{equation}

La función \(f_t\) se corresponde con la puerta de olvido (\textit{Forget Gate}), mientras que 
\(i_t\) con la compuerta de entrada (\textit{Input Gate}) y \(o_t\) con la de salida (\textit{Output
Gate}).

De esta manera, cada una de estas empleará dos matrices de pesos diferentes (\(W\) y \(U\)) en cada caso, que se compartirán
en distintos instantes de tiempo.

Con los valores de las compuertas, posteriormente se calcula un candidato de la cantidad de información
a añadirse:
\begin{equation}
    \widetilde{c}_t = tanh (W_{c} x_t + U_{c} y_{t-1} + b_c) 
\end{equation}

Finalmente, el estado de la red se actualiza siguiendo la ponderación:
\begin{equation}
    c_t = f_t \odot  c_{t-1} + i_{t} \odot \widetilde{c}_t 
\end{equation}

Una vez calculado el nuevo estado, se emplea la puerta de salida, junto con la tangente hiperbólica
del nuevo estado oculto de la celda para obtener la nueva salida de la celda:
\begin{equation}
    y_t =  o_t \odot tanh(c_t)
\end{equation}

En la Figura \ref{fig:lstm_cell_mem.png} puede observarse el diagrama con las operaciones mencionadas
para producir la salida de la celda en el modelo \textit{LSTM}.
\imagen{lstm_cell_mem.png}{Celda de memoria de LSTM}{1}

\subsubsection{GRU}
A diferencia de \textit{LSTM}, este modelo contará con dos compuertas en lugar 
de tres. Estas serán la puerta de \textit{reset} y la de actualización.

Al igual que antes, los valores en la salida de la puerta se calculan de forma similar,
con matrices de pesos diferentes en cada caso.
La puerta de actualización se calcula entonces como:
\begin{equation}
    z_t = \sigma(W_z x_t + U_z y_{t-1} + b_z)
\end{equation}

De forma análoga la puerta de \textit{reset} se obtiene con la fórmula:
\begin{equation}
    r_t = \sigma(W_r x_t + U_r y_{t-1} + b_r)
\end{equation}

De nuevo se calcula la información candidata a añadirse a la memoria:
\begin{equation}
    \widetilde{c}_t = tanh(W_c x_t + U_c (r_t \odot y_{t-1}) + b_c)
\end{equation}

Por último, se calcula el nuevo estado de la celda empleando una media ponderada
entre el estado anterior y la nueva información:
\begin{equation}
    c_t = (1 - z_t) \odot y_{t-1} + z_t \odot \widetilde{c}_t
\end{equation}

Los modelos \textit{GRU} se contemplan como una simplificación de \textit{LSTM},
en la que se fusionan los estados con la respuesta de la red y se fuerza para que estos
sean una media ponderada, llevando a obtener resultados similares que con las redes
mencionadas \cite{book:rue2019}.

\imagen{gru_cell_mem.png}{Celda de memoria de GRU}{1}