{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos para poder ser empleados por la _RNA_\n",
    "\n",
    "Importamos las librerías para el procesamiento de los datos.\n",
    "Para el proceso de limpieza de los datos inválidos y los outliers emplearemos Pandas.\n",
    "\n",
    "Por otro lado, en el caso de los datos del pluviómetro para comprobar los valores anómalos emplearemos la librería nativa de peticiones (requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math as m\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las constantes para establecer la dirección en la que se encuentran los ficheros de datos y el directorio en el que se deberá almacenar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de directorios\n",
    "RAW_FILES_PATH = './data/raw_data'\n",
    "CLASSIFIED_FILES_PATH = './data/classified_data/'\n",
    "SENSOR_FILES_DIRECTORY = RAW_FILES_PATH + '/sensores/'\n",
    "PLUVIOMETER_FILES_DIRECTORY = RAW_FILES_PATH + '/pluviometro/'\n",
    "\n",
    "# URL de API meteorologica para realizar las comparaciones\n",
    "WEATHER_API_BASE_URL = 'https://history.openweathermap.org/data/2.5/history/city?lat=41.6966212&lon=-3.9284274&dt={dt}&units=metric&appid={key}'\n",
    "\n",
    "APIKEY = '9382de5e41593eeb1d830727d6df604d'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detección los valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(data, t_times):\n",
    "    # Ventana de datos agrupados por dia\n",
    "    rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "\n",
    "    for _, group in rolling_win:\n",
    "        # Para cada columna realizamos el analisis por deteccion de valores atipicos\n",
    "        for col in data.columns:\n",
    "            if(col != 'ts'):\n",
    "                # Calculo de la media y la desviacion tipica del grupo\n",
    "                group_mean = group[col].mean()\n",
    "                group_std = group[col].std()\n",
    "                group_median = group[col].median()\n",
    "\n",
    "                # Definicion del umbral para los valores atipicos (se establece x veces la desviacion estandar)\n",
    "                threshold = t_times * group_std\n",
    "\n",
    "                # Identificacion de los valores atipicos\n",
    "                outliers = group[(group[col] - group_mean).abs() > threshold]\n",
    "\n",
    "                data.loc[data['ts'].isin(outliers['ts']), col] = group_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluviometro(data):\n",
    "    with open(PLUVIOMETER_FILES_DIRECTORY + \"estacion_aranda.json\") as f:\n",
    "        # Transformacion del json original (creado como una lista de diccionarios)\n",
    "        # en un diccionario con las fechas como claves y el resto de campos como valores\n",
    "        original_api_data = json.load(f)\n",
    "        api_data = dict()\n",
    "        for sample in original_api_data:\n",
    "            api_data.update({sample[\"fecha\"]: sample})\n",
    "\n",
    "        # Ventana de datos agrupados por dia\n",
    "        rolling_win = data.groupby(pd.Grouper(freq=\"24H\", key=\"ts\"))\n",
    "        for day, group in rolling_win:\n",
    "            # Calculo de la media de las precipitaciones del grupo (precipitaciones\n",
    "            # diarias en litros/m^2)\n",
    "            group_mean = group[\"pluv_deltaMM\"].mean()\n",
    "            \n",
    "            # Obtenemos el dia del grupo como cadena de texto para poder\n",
    "            # emplearlo como clave en el diccionario y asi poder comparar las \n",
    "            # precipitaciones\n",
    "            day = str(day.date())\n",
    "\n",
    "            # Si las precipitaciones medias difieren en 0,5 litros/m^2 establecemos\n",
    "            # las precipitaciones de ese dia a las medias.\n",
    "            if abs(group_mean - float(api_data[day][\"prec\"].replace(\",\", \".\")) * 0.1) > 0.5:\n",
    "                data.loc[data['ts'].isin(group['ts']), \"pluv_deltaMM\"] = api_data[day][\"prec\"] \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los ficheros de los datos relacionados con los sensores debemos realizar operaciones diferentes que en el archivo del pluviómetro, por lo que se divide el procesamiento en varias funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sensor_files = [file for file in os.listdir(SENSOR_FILES_DIRECTORY) if \".csv\" in file]\n",
    "    pluviometer_files = [file for file in os.listdir(PLUVIOMETER_FILES_DIRECTORY) if \".csv\" in file]\n",
    "\n",
    "    sensor_data_processing(sensor_files)\n",
    "    pluviometer_data_processing(pluviometer_files)\n",
    "\n",
    "def sensor_data_processing(sensor_files):\n",
    "    for file in sensor_files:\n",
    "\n",
    "        dataset = pd.read_csv(SENSOR_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminamos los datos marcados como invalidos\n",
    "        dataset = dataset[dataset['validez'] != 0]\n",
    "\n",
    "        # Transformamos los timestamps en datetimes, para posteriormente poder\n",
    "        # agrupar los datos por dia.\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "\n",
    "        # Eliminamos las columnas de fecha, h_C, h_L (por redundancia) y la columna\n",
    "        # de la bateria (no es esencial en la regresion).\n",
    "        # Por otro, lado deshechamos la columna de validez (no es necesaria)\n",
    "        dataset = dataset.drop(['fecha', 'h_C', 'h_L', 'bateria', 'validez'], axis=1)\n",
    "\n",
    "        # Eliminar nulos (data missing)\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 1.2 if file == \"sensor6.csv\" else 2)\n",
    "\n",
    "        dataset['ts'] = pd.to_numeric(dataset['ts'])\n",
    "\n",
    "        dataset.to_csv(CLASSIFIED_FILES_PATH + file, index = False)\n",
    "\n",
    "def pluviometer_data_processing(pluviometer_files):\n",
    "    for file in pluviometer_files:\n",
    "        dataset = pd.read_csv(PLUVIOMETER_FILES_DIRECTORY + file)\n",
    "\n",
    "        # Eliminar campos sin valores\n",
    "        dataset = dataset.dropna()\n",
    "\n",
    "        dataset['ts'] = pd.to_datetime(dataset['ts'], unit=\"ms\")\n",
    "        dataset = dataset.drop(['fecha', 'bateria'], axis=1)\n",
    "\n",
    "        # Eliminar outliers\n",
    "        outlier_detection(dataset, 2)\n",
    "\n",
    "        # Sustitucion de los datos del pluviometro en funcion de las lluvias diarias\n",
    "        pluviometro(dataset)\n",
    "\n",
    "        dataset['ts'] = pd.to_numeric(dataset['ts'])\n",
    "\n",
    "        dataset.to_csv(CLASSIFIED_FILES_PATH + file, index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f22e608a9410c9a00adbb49e3cb6a72010c497ae6b30c9496ff58de178a89c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
